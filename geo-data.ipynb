{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be8dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from dbfread import DBF\n",
    "import folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f822bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DBF attributes only (no geometry): Merged_PCT_Vectorized.dbf\n",
      "(1809807, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DN</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>57.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DN    Area\n",
       "0   8  57.853\n",
       "1   3   4.132\n",
       "2   3   8.265"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base = Path('society-maps/DHA')\n",
    "dbf_path = base / 'Merged_PCT_Vectorized.dbf'\n",
    "if not dbf_path.exists():\n",
    "    raise FileNotFoundError(f\"DBF not found: {dbf_path}\")\n",
    "\n",
    "# Load DBF attributes (no geometry)\n",
    "records = list(DBF(str(dbf_path), load=True))\n",
    "dha = pd.DataFrame(records)\n",
    "print(f'Loaded DBF attributes only (no geometry): {dbf_path.name}')\n",
    "\n",
    "# Quick preview\n",
    "print(dha.shape)\n",
    "dha.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ad109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           geometry\n",
      "0   1  POLYGON ((74.35763 31.4775, 74.38137 31.48744,...\n",
      "1   2  POLYGON ((74.38145 31.48739, 74.38211 31.48971...\n",
      "2   3  POLYGON ((74.36678 31.46315, 74.38894 31.4703,...\n",
      "3   4  POLYGON ((74.39345 31.4538, 74.39259 31.46683,...\n",
      "4   5  POLYGON ((74.44678 31.45479, 74.45226 31.45042...\n",
      "EPSG:4326\n",
      "GeoJSON saved to: society-maps\\DHA\\Split_Areas\\DHA_AREA_split.geojson\n",
      "Map saved to: society-maps\\DHA\\Split_Areas\\dha_map.html\n",
      "Map saved to: society-maps\\DHA\\Split_Areas\\dha_map.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                           geometry\n",
      "0   1  POLYGON ((74.35763 31.4775, 74.38137 31.48744,...\n",
      "1   2  POLYGON ((74.38145 31.48739, 74.38211 31.48971...\n",
      "2   3  POLYGON ((74.36678 31.46315, 74.38894 31.4703,...\n",
      "3   4  POLYGON ((74.39345 31.4538, 74.39259 31.46683,...\n",
      "4   5  POLYGON ((74.44678 31.45479, 74.45226 31.45042...\n",
      "EPSG:4326\n",
      "GeoJSON saved to: society-maps\\DHA\\Split_Areas\\DHA_AREA_split.geojson\n",
      "Map saved to: society-maps\\DHA\\Split_Areas\\dha_map.html\n",
      "Map saved to: society-maps\\DHA\\Split_Areas\\dha_map.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad Hassan\\AppData\\Local\\Temp\\ipykernel_45556\\1462208590.py:29: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  m = folium.Map(location=[gdf.geometry.centroid.y.mean(),\n",
      "C:\\Users\\Ahmad Hassan\\AppData\\Local\\Temp\\ipykernel_45556\\1462208590.py:30: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  gdf.geometry.centroid.x.mean()],\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "split_base = Path(\"society-maps/DHA/Split_Areas\")\n",
    "shp_path = split_base / \"DHA_AREA_split.shp\"\n",
    "\n",
    "# Read shapefile\n",
    "gdf = gpd.read_file(shp_path)\n",
    "\n",
    "# Optional: inspect the data\n",
    "print(gdf.head())\n",
    "print(gdf.crs)  # Check projection\n",
    "\n",
    "# If CRS is not WGS84, convert so folium works properly\n",
    "if gdf.crs and gdf.crs.to_string() != \"EPSG:4326\":\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Save to GeoJSON\n",
    "geojson_path = split_base / \"DHA_AREA_split.geojson\"\n",
    "gdf.to_file(geojson_path, driver=\"GeoJSON\")\n",
    "print(f\"GeoJSON saved to: {geojson_path}\")\n",
    "\n",
    "# Create folium map centered on the data\n",
    "m = folium.Map(location=[gdf.geometry.centroid.y.mean(),\n",
    "                         gdf.geometry.centroid.x.mean()],\n",
    "               zoom_start=14)\n",
    "\n",
    "gdf[\"Name\"] = gdf[\"id\"].astype(str).map(id_mappings) if gdf[\"id\"].astype(str).map(id_mappings).notna().any() else gdf[\"id\"].astype(str)\n",
    "\n",
    "# Add the GeoJSON layer\n",
    "folium.GeoJson(\n",
    "    gdf,\n",
    "    name=\"DHA Areas\",\n",
    "    popup=folium.GeoJsonPopup(fields=[\"Name\"], aliases=[\"Area ID:\"])\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Save map to HTML\n",
    "map_path = split_base / \"dha_map.html\"\n",
    "m.save(map_path)\n",
    "print(f\"Map saved to: {map_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a0ca3",
   "metadata": {},
   "source": [
    "## Folium Mapping for DHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d26050",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mappings = {\n",
    "    \"1\": \"DHA Phase 3\",\n",
    "    \"2\": \"DHA Phase 1,2\",\n",
    "    \"3\": \"DHA Phase 4\",\n",
    "    \"4\": \"DHA Phase 5\",\n",
    "    \"5\": \"DHA Phase 9 Town\",\n",
    "    \"6\": \"Askari 11\",\n",
    "    \"7\": \"DHA Phase 8 - Ex Park View\",\n",
    "    \"8\": \"DHA Phase 8 - Ex Air Avenue\",\n",
    "    \"9\": \"DHA Phase 8\",\n",
    "    \"10\": \"DHA Phase 8\",\n",
    "    \"11\": \"DHA Phase 6\",\n",
    "    \"12\": \"DHA Phase 7\",\n",
    "    \"13\": \"DHA Phase 7\",\n",
    "    \"14\": \"DHA Phase 7\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a90f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved heatmap to dha_price_heatmap.html\n",
      "Examples of partial match checks:\n",
      "- substring in string: True\n",
      "- pandas contains: True\n",
      "- any token: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Load data\n",
    "listings = pd.read_csv(\"./data/zameen_lahore_data.csv\")\n",
    "polys = gpd.read_file(\"society-maps/DHA/Split_Areas/DHA_AREA_split.geojson\")\n",
    "\n",
    "# Ensure polygons are in WGS84 for folium\n",
    "if polys.crs and polys.crs.to_string() != \"EPSG:4326\":\n",
    "    polys = polys.to_crs(epsg=4326)\n",
    "\n",
    "# Map polygon IDs to human-friendly names using provided mapping\n",
    "polys[\"Group\"] = polys[\"id\"].astype(str).map(id_mappings)\n",
    "\n",
    "# Build normalized match list from mapping values\n",
    "# We'll create a mapping from canonical group name -> list of partial keywords\n",
    "canonical_groups = sorted(set([v for v in id_mappings.values() if isinstance(v, str)]))\n",
    "# Expand a few common variants for robustness\n",
    "keyword_expansions = {\n",
    "    \"DHA Phase 1,2\": [\"dha phase 1\", \"dha phase 2\", \"dha phase 1-2\", \"dha phase 1 , 2\"],\n",
    "    \"DHA Phase 9 Town\": [\"dha phase 9 town\", \"phase 9 town\"],\n",
    "    \"DHA Phase 8 - Ex Park View\": [\"dha phase 8 ex park view\", \"ex park view\", \"park view\"],\n",
    "    \"DHA Phase 8 - Ex Air Avenue\": [\"dha phase 8 ex air avenue\", \"ex air avenue\", \"air avenue\"],\n",
    "}\n",
    "\n",
    "# Build search patterns per canonical group (lowercased)\n",
    "search_patterns = {}\n",
    "for name in canonical_groups:\n",
    "    base = [name.lower()]\n",
    "    base += keyword_expansions.get(name, [])\n",
    "    # also add simple \"DHA Phase X\" variant tokens to catch usual text\n",
    "    m = re.findall(r\"phase\\s*\\d+\", name.lower())\n",
    "    base += [f\"dha {t}\" for t in m]\n",
    "    # dedupe\n",
    "    tokens = sorted(set([t.strip() for t in base if t.strip()]))\n",
    "    search_patterns[name] = tokens\n",
    "\n",
    "# Helper to parse price strings to PKR\n",
    "mults = {\"crore\": 10_000_000, \"lac\": 100_000, \"lakh\": 100_000}\n",
    "\n",
    "def parse_price(price_str):\n",
    "    if price_str is None or (hasattr(price_str, '__len__') and len(str(price_str).strip()) == 0):\n",
    "        return None\n",
    "    s = str(price_str).strip().lower()\n",
    "    if s == 'nan' or s == '':\n",
    "        return None\n",
    "    m = re.search(r\"([\\d\\.]+)\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    val = float(m.group(1))\n",
    "    for k, mult in mults.items():\n",
    "        if k in s:\n",
    "            return val * mult\n",
    "    return val\n",
    "\n",
    "if \"Price\" not in listings.columns:\n",
    "    raise KeyError(\"Expected column 'Price' in listings\")\n",
    "\n",
    "listings[\"Price_PKR\"] = listings[\"Price\"].apply(parse_price)\n",
    "\n",
    "# Choose location column\n",
    "loc_col = \"Location\" if \"Location\" in listings.columns else (\"location\" if \"location\" in listings.columns else None)\n",
    "if not loc_col:\n",
    "    raise KeyError(\"Could not find a Location column in listings\")\n",
    "\n",
    "# Assign group by partial match\n",
    "listings[\"Group\"] = None\n",
    "loc_series = listings[loc_col].fillna(\"\").astype(str)\n",
    "for canon, tokens in search_patterns.items():\n",
    "    # vectorized OR of substring contains\n",
    "    mask = pd.Series(False, index=listings.index)\n",
    "    for tok in tokens:\n",
    "        mask |= loc_series.str.lower().str.contains(re.escape(tok), na=False)\n",
    "    listings.loc[mask & listings[\"Group\"].isna(), \"Group\"] = canon\n",
    "\n",
    "# Compute average price per group\n",
    "avg_prices = (\n",
    "    listings.dropna(subset=[\"Group\"]).groupby(\"Group\", as_index=False)[\"Price_PKR\"].mean()\n",
    ")\n",
    "avg_prices.rename(columns={\"Price_PKR\": \"AvgPrice\"}, inplace=True)\n",
    "\n",
    "# Join to polygons and build heatmap\n",
    "polys2 = polys.dropna(subset=[\"Group\"]).merge(avg_prices, on=\"Group\", how=\"left\")\n",
    "\n",
    "# Fallback label if AvgPrice missing\n",
    "# Format prices for display and fill NaN values\n",
    "def format_price(price):\n",
    "    if pd.isna(price) or price == 0:\n",
    "        return \"No Data\"\n",
    "    if price >= 10_000_000:  # 1 crore or more\n",
    "        return f\"{price/10_000_000:.2f} Crore\"\n",
    "    elif price >= 100_000:  # 1 lac or more\n",
    "        return f\"{price/100_000:.2f} Lac\"\n",
    "    else:\n",
    "        return f\"{price:,.0f} PKR\"\n",
    "\n",
    "polys2[\"AvgPrice\"] = polys2[\"AvgPrice\"].fillna(0)\n",
    "polys2[\"FormattedPrice\"] = polys2[\"AvgPrice\"].apply(format_price)\n",
    "\n",
    "m = folium.Map(location=[31.4700, 74.4120], zoom_start=12, tiles=\"cartodbpositron\")\n",
    "\n",
    "ch = folium.Choropleth(\n",
    "    geo_data=polys2.to_json(),\n",
    "    data=polys2,\n",
    "    columns=[\"Group\", \"AvgPrice\"],\n",
    "    key_on=\"feature.properties.Group\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name=\"Average Listing Price (PKR)\",\n",
    ").add_to(m)\n",
    "\n",
    "folium.GeoJsonTooltip(\n",
    "    fields=[\"Group\", \"FormattedPrice\"],\n",
    "    aliases=[\"Area\", \"Avg Price (PKR)\"],\n",
    "    localize=True,\n",
    "    sticky=True,\n",
    ").add_to(ch.geojson)\n",
    "\n",
    "m.save(\"dha_price_heatmap.html\")\n",
    "print(\"Saved heatmap to dha_price_heatmap.html\")\n",
    "\n",
    "# Also show several partial-match approaches for reference\n",
    "print(\"Examples of partial match checks:\")\n",
    "example_loc = \"10 Marla House in DHA Phase 8 - Ex Air Avenue\"\n",
    "print(\"- substring in string:\", \"DHA Phase 8\".lower() in example_loc.lower())\n",
    "print(\"- pandas contains:\", pd.Series([example_loc]).str.contains(\"phase\\\\s*8\", case=False, regex=True).iloc[0])\n",
    "print(\"- any token:\", any(tok in example_loc.lower() for tok in search_patterns[\"DHA Phase 8 - Ex Air Avenue\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855c2b6",
   "metadata": {},
   "source": [
    "## Extraction of Lahore Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3bd1684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the unified Lahore boundary file: Lahore_Boundary.geojson\n",
      "Successfully created the Lahore tehsils file: Lahore_Tehsils.geojson\n",
      "Found 0 tehsils in Lahore.\n",
      "Tehsils found: []\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths to the uploaded files\n",
    "district_file = Path(\"geo-data/lahore-boundary/District Punjab.geojson\")\n",
    "tehsil_file = Path(\"geo-data/lahore-boundary/Tehsil Punjab.geojson\")\n",
    "\n",
    "districts_gdf = gpd.read_file(district_file)\n",
    "\n",
    "lahore_district = districts_gdf[districts_gdf['DistrictCo'] == 'Lahore']\n",
    "\n",
    "\n",
    "lahore_boundary = lahore_district.dissolve()\n",
    "output_lahore_boundary_file = Path(\"Lahore_Boundary.geojson\")\n",
    "lahore_boundary.to_file(output_lahore_boundary_file, driver='GeoJSON')\n",
    "print(f\"Successfully created the unified Lahore boundary file: {output_lahore_boundary_file}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tehsils_gdf = gpd.read_file(tehsil_file)\n",
    "lahore_tehsils = tehsils_gdf[tehsils_gdf['DistrictCo'] == 'Lahore']\n",
    "\n",
    "\n",
    "output_lahore_tehsils_file = Path(\"Lahore_Tehsils.geojson\")\n",
    "lahore_tehsils.to_file(output_lahore_tehsils_file, driver='GeoJSON')\n",
    "\n",
    "print(f\"Successfully created the Lahore tehsils file: {output_lahore_tehsils_file}\")\n",
    "print(f\"Found {len(lahore_tehsils)} tehsils in Lahore.\")\n",
    "\n",
    "# Assuming 'TEHSIL_N' or 'Tehsil_Nam' is the correct column for Tehsil names based on inspection\n",
    "# If this next line gives an error, find the right tehsil name column in the printed list above.\n",
    "if 'TehsilName' in lahore_tehsils.columns:\n",
    "    print(\"Tehsils found:\", list(lahore_tehsils['TehsilName']))\n",
    "elif 'Tehsil_Nam' in lahore_tehsils.columns:\n",
    "     print(\"Tehsils found:\", list(lahore_tehsils['Tehsil_Nam']))\n",
    "else:\n",
    "    print(\"Could not determine the Tehsil name column. Please inspect the printed column list.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
