{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58e6a25",
   "metadata": {},
   "source": [
    "## Scraping off of [Graana.com](https://www.graana.com) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1c1441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data from data/graana_lahore_data.csv...\n",
      "Loaded 1964 existing URLs. Will skip any duplicates.\n",
      "--- Starting to scrape Graana.com (Lahore) ---\n",
      "Fetching Commercial for Sale from: https://www.graana.com/sale/commercial-properties-sale-lahore-2/?page=1&pageSize=30\n",
      "Processing listing URL: https://www.graana.com/property/300-sqft-shop-sale-bismillah-housing-scheme-lahore-1466928/\n",
      "Processing listing URL: https://www.graana.com/property/1100-sqft-plaza-sale-bahria-town---sector-e--lahore-1466682/\n",
      "Processing listing URL: https://www.graana.com/property/190-sqft-office-sale-anarkali-lahore-1464300/\n",
      "Processing listing URL: https://www.graana.com/property/2.4-marla-shop-sale-township-lahore-1462300\n",
      "Processing listing URL: https://www.graana.com/property/7-marla-office-sale-audit-and-accounts-housing-society-lahore-1460806/\n",
      "Processing listing URL: https://www.graana.com/property/489-sqft-office-sale-gulberg-3-lahore-1456759/\n",
      "Processing listing URL: https://www.graana.com/property/10-marla-building-sale-anarkali-lahore-1454693/\n",
      "Processing listing URL: https://www.graana.com/property/480-sqft-shop-sale-mazang-lahore-1452237/\n",
      "Processing listing URL: https://www.graana.com/property/6-marla-building-sale-samanabad-lahore-1448705/\n",
      "Processing listing URL: https://www.graana.com/property/8-marla-plaza-sale-dha-phase-8-cbw-lahore-1447478/\n",
      "Processing listing URL: https://www.graana.com/property/39-marla-building-sale-samanabad-lahore-1444092/\n",
      "Processing listing URL: https://www.graana.com/property/31-marla-factory-sale-kot-lakhpat-lahore-1443786/\n",
      "Processing listing URL: https://www.graana.com/property/14-marla-plaza-sale-raiwind-road-lahore-1441800/\n",
      "Processing listing URL: https://www.graana.com/property/280-sqft-office-sale-awan-town-lahore-1439663/\n",
      "Processing listing URL: https://www.graana.com/property/2515-sqft-office-sale-allama-iqbal-town---asif-block-lahore-1439615/\n",
      "Processing listing URL: https://www.graana.com/property/2-marla-land-sale-marghzar-colony-lahore-1439586/\n",
      "Processing listing URL: https://www.graana.com/property/5.75-marla-plaza-sale-empress-road-lahore-1438671\n",
      "Processing listing URL: https://www.graana.com/property/18-marla-plaza-sale-davis-road-lahore-1438669/\n",
      "Processing listing URL: https://www.graana.com/property/6-marla-building-sale-mustafa-town-lahore-1437801/\n",
      "Processing listing URL: https://www.graana.com/property/14-marla-building-sale-gulberg-3-lahore-1436652/\n",
      "Processing listing URL: https://www.graana.com/property/6-marla-shop-sale-dha-phase-6-lahore-1436649/\n",
      "Processing listing URL: https://www.graana.com/property/4.55-marla-office-sale-gulberg-2-lahore-1436103\n",
      "Processing listing URL: https://www.graana.com/property/595-sqft-office-sale-dha-phase-2-lahore-1431655/\n",
      "Processing listing URL: https://www.graana.com/property/81-sqft-shop-sale-model-town-extension-lahore-1430000/\n",
      "Processing listing URL: https://www.graana.com/property/42-sqft-shop-sale-anarkali-lahore-1429068/\n",
      "Processing listing URL: https://www.graana.com/property/2-marla-building-sale-mustafa-town-lahore-1425766/\n",
      "Processing listing URL: https://www.graana.com/property/2-marla-building-sale-mustafa-town-lahore-1425764/\n",
      "Processing listing URL: https://www.graana.com/property/21-marla-building-sale-gulberg-3-lahore-1417944/\n",
      "Processing listing URL: https://www.graana.com/property/2-kanal-land-sale-samanabad-lahore-1417187/\n",
      "Processing listing URL: https://www.graana.com/property/128-sqft-shop-sale-mm-alam-road-lahore-1305611/\n",
      "Fetching Commercial for Sale from: https://www.graana.com/sale/commercial-properties-sale-lahore-2/?page=2&pageSize=30\n",
      "Processing listing URL: https://www.graana.com/property/8-kanal-warehouse-sale-multan-road-lahore-1270049/\n",
      "Processing listing URL: https://www.graana.com/property/12000-sqft-warehouse-sale-multan-road-lahore-1270000/\n",
      "Processing listing URL: https://www.graana.com/property/24000-sqft-warehouse-sale-multan-road-lahore-1029950/\n",
      "Processing listing URL: https://www.graana.com/property/2-marla-building-sale-dha-phase-8-lahore-659086/\n",
      "Fetching Commercial for Sale from: https://www.graana.com/sale/commercial-properties-sale-lahore-2/?page=3&pageSize=30\n",
      "No more Commercial for Sale listings found on page 3. Moving to next category.\n",
      "\n",
      "Scraping completed. Total listings to write: 1964.\n",
      "Data successfully exported to data/graana_lahore_data.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape_graana_lahore(output_csv='data/graana_lahore_data.csv', max_pages=30):\n",
    "    \"\"\"\n",
    "    Scrapes property listings from Graana.com for Lahore.\n",
    "\n",
    "    WARNING: For educational purposes only. Web scraping can be against the\n",
    "    terms of service of a website. Please respect Graana.com's policies.\n",
    "    Frequent, rapid requests can get your IP address blocked.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URLs for different categories in Lahore on Graana.com\n",
    "    # The structure is typically /lahore/{category}?page=\n",
    "    base_url_sale_houses = \"https://www.graana.com/sale/residential-properties-sale-lahore-2/?pageSize=30&page={}\"\n",
    "    base_url_sale_plots = \"https://www.graana.com/sale/plot-sale-lahore-2/?page={}&pageSize=30\"\n",
    "    base_url_commercial = \"https://www.graana.com/sale/commercial-properties-sale-lahore-2/?page={}&pageSize=30\"\n",
    "    base_url_rentals_commercial = \"https://www.graana.com/rent/commercial-properties-rent-lahore-2/?page={}&pageSize=30\"\n",
    "    base_url_rentals_residential = \"https://www.graana.com/rent/residential-properties-rent-lahore-2/?page={}&pageSize=30\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    all_listings = []\n",
    "    existing_urls = set()\n",
    "\n",
    "    # --- Step 1: Load existing data and URLs from CSV if it exists ---\n",
    "    if os.path.exists(output_csv):\n",
    "        print(f\"Loading existing data from {output_csv}...\")\n",
    "        try:\n",
    "            with open(output_csv, 'r', newline='', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for row in reader:\n",
    "                    # Append existing data to our list\n",
    "                    all_listings.append(row)\n",
    "                    # Add the URL to our set for duplicate checking\n",
    "                    if 'URL' in row and row['URL']:\n",
    "                        existing_urls.add(row['URL'])\n",
    "            print(f\"Loaded {len(existing_urls)} existing URLs. Will skip any duplicates.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read existing CSV file. Starting fresh. Error: {e}\")\n",
    "            all_listings = [] # Reset list if the file is corrupt or unreadable\n",
    "\n",
    "    \n",
    "    def fetch_listings(url_template, category):\n",
    "        \"\"\"Fetches and parses listings for a given category.\"\"\"\n",
    "        page_num = 1\n",
    "        while page_num <= max_pages:\n",
    "            url = url_template.format(page_num)\n",
    "            print(f\"Fetching {category} from: {url}\")\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=15)\n",
    "                response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "                break # Stop this category if a page fails to load\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # The main container for each property listing\n",
    "            listings = soup.find_all('div', class_='MuiBox-root mui-style-17zbhp0')\n",
    "\n",
    "            if not listings:\n",
    "                print(f\"No more {category} listings found on page {page_num}. Moving to next category.\")\n",
    "                break\n",
    "\n",
    "            for listing in listings:\n",
    "                try:\n",
    "                    # Find the link tag which contains the URL and the title\n",
    "                    link_tag = listing.find('a')\n",
    "                    \n",
    "                    if not link_tag:\n",
    "                        continue # Skip if no link tag is found\n",
    "\n",
    "                    # --- Step 2: Construct full URL and check for duplicates ---\n",
    "                    relative_link = link_tag.get('href', '')\n",
    "                    full_url = \"https://www.graana.com\" + relative_link\n",
    "                    print(f\"Processing listing URL: {full_url}\")\n",
    "\n",
    "                    \n",
    "                    if full_url in existing_urls:\n",
    "                        # print(f\"Skipping duplicate: {full_url}\")\n",
    "                        continue # Skip to the next listing if URL is already known\n",
    "                    \n",
    "                    # Open the listing URL to extract the title from the detail page\n",
    "                    try:\n",
    "                        detail_response = requests.get(full_url, headers=headers, timeout=15)\n",
    "                        detail_response.raise_for_status()\n",
    "                        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
    "                        # print(f\"Detail soup sample: {detail_soup.title.string if detail_soup.title else 'No title found'}\")\n",
    "                        # Try to extract the title from the detail page\n",
    "                        # detail_title_tag = detail_soup.find('h1', class_='MuiTypography-root MuiTypography-h3New mui-style-s6x0cd')\n",
    "                        title = detail_soup.title.string.strip() if detail_soup.title else 'N/A'\n",
    "                        # print(f\"Extracted title from detail page: {detail_soup.title.text.strip()}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Could not fetch title from detail page {full_url}: {e}\")\n",
    "                        title = link_tag.text.strip() if link_tag else 'N/A'\n",
    "                    # title = link_tag.text.strip() if link_tag else 'N/A'\n",
    "\n",
    "                    price_tag = listing.find('div', class_='MuiTypography-root MuiTypography-h4New mui-style-gz23my')\n",
    "                    price = price_tag.text.strip() if price_tag else 'N/A'\n",
    "                    \n",
    "                    location_tag = listing.find('h5')\n",
    "                    location = location_tag.text.strip() if location_tag else 'N/A'\n",
    "\n",
    "                    # Area is often found in a container with other features\n",
    "                    area_tag = listing.find('div', class_='MuiTypography-root MuiTypography-body2New mui-style-1548769')\n",
    "                    area = area_tag.text.strip() if area_tag else 'N/A'\n",
    "                    \n",
    "                    print(f\"Found new listing: {title}, Price: {price}, Location: {location}, Area: {area}\")\n",
    "\n",
    "                    # Add the new, unique listing to our list\n",
    "                    all_listings.append({\n",
    "                        'Category': category,\n",
    "                        'Title': title,\n",
    "                        'Price': price,\n",
    "                        'Location': location,\n",
    "                        'Area': area,\n",
    "                        'URL': full_url\n",
    "                    })\n",
    "\n",
    "                    # --- Step 3: Add the new URL to the set to avoid duplicates in this session ---\n",
    "                    existing_urls.add(full_url)\n",
    "\n",
    "                except AttributeError as e:\n",
    "                    print(f\"Skipping a listing due to a missing element: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An unexpected error occurred while processing a listing: {e}\")\n",
    "\n",
    "            page_num += 1\n",
    "            # Be respectful to the server by waiting between requests\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "\n",
    "    print(\"--- Starting to scrape Graana.com (Lahore) ---\")\n",
    "    \n",
    "    # Scrape each category\n",
    "    # fetch_listings(base_url_sale_houses, 'Homes for Sale')\n",
    "    # fetch_listings(base_url_sale_plots, 'Plots for Sale')\n",
    "    # fetch_listings(base_url_rentals_residential, 'Rentals')\n",
    "    # fetch_listings(base_url_rentals_commercial, 'Commercial Rentals')\n",
    "    fetch_listings(base_url_commercial, 'Commercial for Sale')\n",
    "\n",
    "    print(f\"\\nScraping completed. Total listings to write: {len(all_listings)}.\")\n",
    "\n",
    "    # --- Step 4: Write all data (old and new) to the CSV file ---\n",
    "    if all_listings:\n",
    "        # Get the keys from the first dictionary in the list\n",
    "        keys = all_listings[0].keys()\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(all_listings)\n",
    "        print(f\"Data successfully exported to {output_csv}\")\n",
    "    else:\n",
    "        print(\"No data was scraped to export.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can change the number of pages to scrape here\n",
    "    scrape_graana_lahore(max_pages=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
